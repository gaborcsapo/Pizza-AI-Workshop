{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Intro\n",
    "Let's build a common understanding of machine learning by coding up the same decision tree in the traditional way and in the machine learning way.\n",
    "\n",
    "---\n",
    "\n",
    "## Traditional rule based systems / Software 1.0\n",
    "The process is straightforward. The software engineer writes the rules based on which the system is making decisions. Then runs the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns True or False depending on whether variable a and b are smaller than 15 and 20 respectively.\n",
    "\n",
    "# Decision rules.\n",
    "def make_decision(a, b):\n",
    "    if (a < 15):\n",
    "        if (b < 20):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if (b < 20):\n",
    "            return False\n",
    "        else:\n",
    "            return True \n",
    "        \n",
    "#print output based on rules\n",
    "make_decision(10, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning systems / Software 2.0\n",
    "Software engineer writes a program that based on past examples of decisions figures out the rules of decision making by itself. The process is more involved. Steps:\n",
    "1. Collection of past examples of decision (training data)\n",
    "2. Clean the data, transform it into the format your algorithm likes (binary classes, vectors, numbers)\n",
    "3. Train then test your algorithm and fine-tune it\n",
    "4. Run predictions on your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import machine learning algorithm\n",
    "from sklearn import tree\n",
    "\n",
    "# Steps 1 and 2. Training data (past decision examples)\n",
    "train_x = [[10, 34], [9, 4], [45, 20], [14, 20], [15, 20], [14, 19], [15, 19], [22, 17]] # variables a and b\n",
    "train_y = [ False,    True,    True,     False,    True,     False,    False,    False]  # the decisions made \n",
    "\n",
    "# Step 3. Train ML algo (make your computer figure out the decision rules behind the training data)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 4. Use trained model to make predictions\n",
    "clf.predict([[2, 2], [16, 17], [15, 20], [1, 32]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "&nbsp; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c85876b-1b19-4c50-8330-a2c9dbd91f32",
    "_uuid": "3cae972238580fe9c4fb6a184b3661b232f77458"
   },
   "source": [
    "&nbsp; \n",
    "\n",
    "&nbsp; \n",
    "\n",
    "---\n",
    "### That was a very simple example, so let's take one more step forward.\n",
    "\n",
    "# Let's explore the process of machine learning through a fictional story!\n",
    "\n",
    "&nbsp; \n",
    "\n",
    "![title](picture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Arstotzka!\n",
    "### Things aren't going well and the country's food supplies are running short. \n",
    "### If citizens want to eat, they have to send a letter to the Department of Food Supplies. The officials judge their requests and either reject them or send a slice of pizza.\n",
    "\n",
    "### You're the department's software engineer and your boss told you automate request approval with this new thing called machine learning... Your boss gave you the past decisions in the file, data.csv and new requests in the file new_data.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 1 - Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### As you can see, the first three columns are just duplicates, so let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The table is too big to see. Let's pretty print the 1018th row!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 1017\n",
    "print('UID:\\n', df['request_id'][i], '\\n')\n",
    "print('Title:\\n', df['request_title'][i], '\\n')\n",
    "print('Text:\\n', df['request_text_edit_aware'][i], '\\n')\n",
    "print('Received:\\n', df['requester_received_pizza'][i], '\\n')\n",
    "print('Requester interests:\\n', df['requester_interests'][i].replace(\"'\", \"\"), '\\n')\n",
    "print('Account age:\\t', df['account_age'][i], '\\n')\n",
    "print('Requester city:\\t', df['requester_city'][i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's investigate how many people actually received pizzas in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Ooops, there must be a mistake in the data, which reminds me that we should drop N/A values, and we should only work with rows that have True/False values in their \"requester_received_pizza\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24d5a591-79a6-4740-8b58-c76fef459355",
    "_uuid": "d231a3cdad2807893f44de861892093d02cb07a7"
   },
   "source": [
    "### Summary of fields:\n",
    "\n",
    "**Input data**:\n",
    " - `request_id`: unique identifier for the request \n",
    " - `request_title`: title of the reddit post for pizza request\n",
    " - `request_text_edit_aware`: expository to request for pizza\n",
    " - `requester_interests`: collected tags on what the interests of the requester are\n",
    " - `account_age`: how old is the account\n",
    " - `requester_city`: city requester is from\n",
    " \n",
    "**Output decision made**:\n",
    " - `requester_recieved_pizza`: whether requester gets his/her pizza\n",
    " \n",
    "For our purpose let's choose the request text, interests and city as features to predict whether a person should receive a pizza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2 - Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b869481f-c713-4aa6-aa0d-f2e5f4b2abfc",
    "_uuid": "a2cacb2270490a92ec9a7e920e8ca17c730c1e09"
   },
   "source": [
    "### Split training data into training and validation sets\n",
    "\n",
    "The first thing to do is to split our training data into 2 parts:\n",
    "\n",
    " - **training**: Use for training our model\n",
    " - **validation**: Use to check the \"soundness\" of our model. Run trained algorithm on the data and check how many did our model get right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b61f317-079b-4253-b94a-a87b71bd47fb",
    "_uuid": "ac1e8766dcf5d7d81fe4fd3a95e7bc659fbf1978"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8677912b-d9e2-4fd8-b722-acb3320af4fc",
    "_uuid": "f97576104bbabfc720fa0794ab6a8fc4e043f109"
   },
   "source": [
    "### Small detour to Data formats and Countvectorizing\n",
    "Machine learning algorithms in most cases don't know what to do with characters and strings, so we'll need to transform our strings. One way to do so is countvectorization. But what is that? Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "# create countvectorizer object and string to countvectorize\n",
    "count_vect_text = CountVectorizer()\n",
    "myString = [\"She sells sea shells. The shells she sells are surely sea shells.\"]\n",
    "\n",
    "# the count vectorizer just counts the frequency of each unique word in the sequence and puts it in a dictionary.\n",
    "# the output is in the format:   (<ignore this>, <indx in the string>)  <number of times it occurs>\n",
    "print(count_vect_text.fit_transform(myString))\n",
    "\n",
    "# therefore from the result below we see that for instance:\n",
    "#  - the words at positions 5,0 and 6 correspondinging to \"She\", \"sells\", \"surely\" in the string occur once \n",
    "#  - the one in the 4th position occurs 3 times referring to \"shells\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's countvectorize our whole training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02efe9a8-00e4-41fa-9b6a-5fbebe751feb",
    "_uuid": "4549079bfedbbff679ccde5f083b3c6814e73233",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "stopwords = [\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"    with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]\n",
    "#\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\"\n",
    "    \n",
    "train_y = train['requester_received_pizza']\n",
    "valid_y = valid['requester_received_pizza']    \n",
    "\n",
    "count_vect_text = CountVectorizer(stop_words=stopwords, max_features=4000, min_df=6)\n",
    "train_x = coo_matrix(count_vect_text.fit_transform(train['request_text_edit_aware']))\n",
    "valid_x = coo_matrix(count_vect_text.transform(valid['request_text_edit_aware']))\n",
    "print(train_x.shape)\n",
    "\n",
    "count_vect_int = CountVectorizer(max_features=500, min_df=3)\n",
    "train_x_int = coo_matrix(count_vect_int.fit_transform(train['requester_interests']))\n",
    "valid_x_int = coo_matrix(count_vect_int.transform(valid['requester_interests']))\n",
    "print(train_x_int.shape)\n",
    "\n",
    "count_vect_city = CountVectorizer()\n",
    "train_x_city = coo_matrix(count_vect_city.fit_transform(train['requester_city']))\n",
    "valid_x_city = coo_matrix(count_vect_city.transform(valid['requester_city']))\n",
    "print(train_x_city.shape)\n",
    "\n",
    "train_x = hstack([train_x, train_x_int, train_x_city]).toarray()\n",
    "valid_x = hstack([valid_x, valid_x_int, valid_x_city]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d7ae411-ae2c-4098-beb2-ac6c2dc267a9",
    "_uuid": "3aaf138395ccdfe7a29d89d1b58da9db96ae8b36"
   },
   "source": [
    "---\n",
    "# Step 3 - Training model\n",
    "Finally, we arrived to the part everyone was waiting for! In this step, we'll just test a few different classifiers to see how they perform.\n",
    "\n",
    "### Let's try first a Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a159538-c58a-485d-bc23-d7115ee55a77",
    "_uuid": "44cee633360c7989a3d67716553ef4e2bc9eae87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c2776b5-6515-4321-a7e7-f925534241d6",
    "_uuid": "fbec702a6003287ea5d49049be532ee96b6d7a14"
   },
   "source": [
    "### Let's get a sense of how good our classifier is on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0bba339-cd62-46e9-9106-55ef15000dc5",
    "_uuid": "b5f7b1ac26a30b568d3f5dec2a3f76c34a316ee5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try another algorithm, Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get better results with the SVM, but oftentimes it helps to tweak some parameters of the algorithm. I played around to find the best combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6013d400-4a01-438a-9afe-b540b6ec78b9",
    "_uuid": "dbf9eb2a3dde29a26ae51df05f120ee26334d0c3"
   },
   "source": [
    "---\n",
    "# Step 4 - Run predictions on new data\n",
    "Now we take the data that we have to make decisions on. This dataset has all the features and the algorithm will decide whether these people will get pizzas or not based on the rules it learnt from past decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2fe3ff2b-5157-43b2-ad8d-55389ad28a0e",
    "_uuid": "450827a713f7eaaa2d3c3504b038a513c27b3b20",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('new_data.csv')\n",
    "test = test.fillna(\"\")\n",
    "test_x = coo_matrix(count_vect_text.transform(test['request_text_edit_aware']))\n",
    "test_x_int = coo_matrix(count_vect_int.transform(test['requester_interests']))\n",
    "test_x_city = coo_matrix(count_vect_city.transform(test['requester_city']))\n",
    "test_x = hstack([test_x, test_x_int, test_x_city]).toarray()\n",
    "\n",
    "predictions = clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef6b8e68-46be-4944-b809-872ddeabf386",
    "_uuid": "51a804beaf030dbb8cbdfc320d6a26d86991d274"
   },
   "source": [
    "**Note:** Since we don't have the `requester_received_pizza` field in test data, we can't measure accuracy. But we can do some exploration as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80300555-265f-4557-8b52-17c343a34c58",
    "_uuid": "4ad28c27f351de96d40b68aed92217b17f71c542",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Very niiice!\n",
    "Out of the 1600 new requests, our algorithm rejected 1303 and approved 328, which is similar to the original manual decisions. \n",
    "\n",
    "**Now the government of Arstotzka can fire all its employees, and only employ this one algorithm! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 4+1 - Assume something is wrong and find it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's concat the predictions and features into one table, and select the rows that received a pizza\n",
    "pred_df = pd.concat([test, pd.Series(predictions)], axis=1)\n",
    "received_pizza = pred_df[pred_df[0] == 'True']\n",
    "\n",
    "# as a random guess let's look at where are the people with pizzas from?\n",
    "print(\"Cities of pizza receivers: \\n\",received_pizza['requester_city'].value_counts())\n",
    "\n",
    "# let's look at how many people are from those cities in general\n",
    "print(\"\\nTotal requests from each city: \\n\",pred_df['requester_city'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do you see something wrong?\n",
    "\n",
    "Nirsk and Vescillo have roughly the same population, but for some reason Nirsk received 3.4 times more pizza!! We'll get back to this below.\n",
    "\n",
    "### This was a lot, right? Let's recap what we just did!\n",
    "1. short intro into Machine Learning\n",
    "- we moved onto a bigger project\n",
    "- loaded pizza data\n",
    "- split into training and test sets\n",
    "- vectorized text\n",
    "- tested different models (Naive bayes, SVM)\n",
    "- saw that there is a bias in the results towards one city\n",
    "\n",
    "It is crucial in machine learning to understand the biases in the training dataset, because the algorithms will only learn and reinforce those. \n",
    "\n",
    "[A real life example](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) is when people used an algorithm to decide whether a prison inmate should be pardonned. The problem was that the judges were racially biased towards white people and therefore the algorithm also learned that black people should stay in prison longer.\n",
    "\n",
    "In our fictional Arstotzka example, the people making the original decision might be from Nirsk and sympathized with those people more. However, this is not the behaviour we want our algorithm to have.\n",
    "\n",
    "The same kind of bias in real world datasets has serious consequences, when they are applied to university admissions, insurance, loans and criminal predictions. Data science is more than just numbers, and engineers have the responsibility to find the biases and fix them. [These scientists managed to fix gender bias in the Google Word2Vec model](https://arxiv.org/pdf/1607.06520.pdf).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
